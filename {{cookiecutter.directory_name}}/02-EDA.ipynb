{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {{cookiecutter.project_name}} - EDA, feature engineering and feature importance\n",
    "\n",
    "{{cookiecutter.short_description}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A EDA cheatsheet is in https://github.com/cmawer/pycon-2017-eda-tutorial/blob/master/EDA-cheat-sheet.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "- `cleaned_data.pkl`: Data cleaned in notebook 01_data_cleaning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names mean the following: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog\n",
    "- {% now 'utc', '%m-%d-%Y' %} : Started project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General \n",
    "from datatime import datetime\n",
    "import numpy as np \n",
    "import os \n",
    "import pickle\n",
    "\n",
    "# Plotting \n",
    "import hvplot.pandas\n",
    "%matplotlib inline\n",
    "import altair as alt\n",
    "# for the notebook only (not for JupyterLab) run this command once per session\n",
    "alt.renderers.enable('notebook')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "\n",
    "# EDA \n",
    "import missingno as msno\n",
    "import pandas_profiling\n",
    "import shap\n",
    "\n",
    "# ML \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_file = os.path.join('data', 'processed', 'cleaned_data.pkl')\n",
    "df = pd.read_pickle(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, it is useful to follow the recommendations given by the profiler and remove columns or take some log transforms if the distributions are skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hvplot.scatter_matrix(df) # using c=\"column name\" you can add one more dimension to the plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the information here to decide which transformations you need to perform:\n",
    "* log transform for skewed data\n",
    "* binning might be useful for continuos variables\n",
    "* simplefying categories by aggregating them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with duplicates and missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data can by itself give insight (http://joss.theoj.org/papers/10.21105/joss.00547) this is the reason why one might first want to visulalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.replace('nan', np.nan)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "msno.matrix(df, labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot nullity correlations\n",
    "msno.dendrogram(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have the powerful [pyod](https://github.com/yzhao062/pyod) library in the conda enviornment. You might want to check it out for more complex anonmaly or outlier detection problems. There you can also visualize and combine different approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove outliers base on z score, everything with z >= z_score_cutoff is considered as outlier and will be dropped\n",
    "z_score_cutoff = 3\n",
    "\n",
    "z = np.abs(stats.zscore(df._get_numeric_data(), axis=1)))\n",
    "df_no_outlier = df.loc[list(np.where(z < z_score_cutoff)[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Possible additional steps:\n",
    "* if you have categorical data, you might want to use one-hot encoding `pd.get_dummies(df)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split Normalize/standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[]\n",
    "y = df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, train_size=0.6, random_state=0)\n",
    "scaling = StandardScaler() # or use minmaxscaler or normalizer\n",
    "X_train = scaling.fit_transform(X_train)\n",
    "X_test = scaling.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap analysis of feature importance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shap values are a consistent way to define global and local feature importance to a model (https://christophm.github.io/interpretable-ml-book/shapley.html).\n",
    "We train a gradient boost model, which often performs well, and then use Shap to explain how the model makes its decisions. \n",
    "\n",
    "Note that Shap values are in general a lot more consistent and valuable are the simple estimates you get from tree-based models based on how early/close to the stem a split happend. \n",
    "\n",
    "The shap analysis can also inform further feature engineering steps (e.g. dropping meaningless columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'learning_rate': [0.001, 0.01, 0.05, 0.1], \n",
    "              'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "            }\n",
    "\n",
    "xgb = xgboost.XGBRegressor(n_estimators=600)\n",
    "xgb_grid = GridSearchCV(xgb, parameters, cv=10) # cp. arXiv:1811.12808 another method might be better in your case\n",
    "\n",
    "xgb_grid.fit(X_train, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgb_grid.best_estimator_)\n",
    "shap_values = explainer.shap_values(X)\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a summary plot\n",
    "* features are ordered by importance\n",
    "* color coding gives the feature value\n",
    "* the x-axis shows the influence on the model outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a interaction plot\n",
    "* Shap can also be used to study if there are interactions between variables, these show up in the off-diagonals of the interaction plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interaction_values = explainer.shap_interaction_values(X)\n",
    "shap.summary_plot(interaction_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a force plot\n",
    "* The force plot is a interesting interactive plot:\n",
    "    * The x column you select is used to order the samples\n",
    "    * the y values give the Shap value (influence on the model outcome)\n",
    "    * the color coding shows for each data point the contributions of the different features to the model outcome (use hover)\n",
    "    * the force plots are introduced in [a nature paper of the shap developer](https://www.nature.com/articles/s41551-018-0304-0) and explained [in the readme on the github site](https://github.com/slundberg/shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(shap.force_plot(explainer.expected_value, shap_values, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate single variables in detail "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependendence plots plot the feature value vs. the shap value of the feature. Color coded vertical dispersion is due to interaction effects.\n",
    "The shap library automatically selects the feature for the color coding (the one with the highest interaction).\n",
    "\n",
    "In case you see an interaction in a dependence plot ([a nice example is in the NHANES example notebook](https://github.com/slundberg/shap/blob/master/notebooks/tree_explainer/NHANES%20I%20Survival%20Model.ipynb)) you might want to plot the interaction values with \n",
    "\n",
    "```\n",
    "shap.dependence_plot(\n",
    "    (\"X\", \"Color\"), # select column names for x axis and color \n",
    "    shap_interaction_values, X,\n",
    "    display_features=X,\n",
    ")\n",
    "```\n",
    "\n",
    "i.e. a plot like \n",
    "\n",
    "```\n",
    "shap.dependence_plot(\n",
    "    (\"Age\", \"Age\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")\n",
    "```\n",
    "\n",
    "should have now vertical dispersion since vertical dispersions are due to interactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(0, shap_values, X) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cof_project)",
   "language": "python",
   "name": "cof_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
